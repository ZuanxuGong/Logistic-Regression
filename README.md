# Logistic-Regression
Write and train a Logistic regression using Gradient Descent for Hand-written digit classification (MNIST).

## Data
Data contains 6 files: <br>
1) trainingData.txt <br> 
2) trainingLabels.txt <br> 
3) validationData.txt <br> 
4) validationLabels.txt <br> 
5) testData.txt <br> 
6) testLabels.txt <br>

## Data format
### â€œ*Data.txtâ€ files contain features.
Each row is comma-separated 784 integers which are features of a single sample. <br>
The number of rows (=number of samples) for training, validation, and test data are 6107, 6107 and 2037 respectively. <br>
### â€œ*Labels.txtâ€ files contain +1 or -1 labels. 
Each row corresponds to the corresponding row of the feature file. 
 
## Steps
- Implement the Gradient Descent algorithm in plain numpy. The core of Gradient Descent is simply the following line:
ğ‘¤(ğ‘¡ + 1) â† ğ‘¤(ğ‘¡) âˆ’ ğœ‚ ğ›»ğ‘“(ğ‘¤(ğ‘¡)) where ğ‘“(ğ‘¤(ğ‘¡)) = 1/ğ‘ * âˆ‘ğ‘– ğ¿(ğ‘¥ğ‘–, ğ‘¦ğ‘–; ğ‘¤) is the empirical risk.
- Assume initial w is all zeros, the total number of iterations T is 1000, and learning rate ğœ‚ is a constant.
- Cross-validation. Training with different values of Î¼ = {0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 0.6, 1, 1.3, 3, 10}.
- The best value ğœ‚ = 1.
- The test error is 0.119522. <br> 
PS: remember to change the path.
